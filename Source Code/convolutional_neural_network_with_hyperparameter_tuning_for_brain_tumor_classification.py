# -*- coding: utf-8 -*-
"""Convolutional Neural Network with Hyperparameter Tuning for Brain Tumor Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13QDSwTnf5PucrFGNTfXGJQF1X9Hg-Rwl

Moch Hazmi Cokro Mandiri

**Dataset : [Brain MRI Dataset](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri) dari Kaggle**

**Deskripsi Data :** \
Total Data Images MRI : 3264 Images

Alokasi ulang ke 80% Train | 10% Val  | 10%Test
> Training Data (2609 Images) \
> Validation Data (325 Images) \
> Testing Data (330 Images)

###Mount Gdrive serta Download dan Ekstrak Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Dataset/Dataset Citra 240 - 235

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri

!mkdir 'Dataset Brain MRI'

import zipfile
ekstrak_zip = '/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/brain-tumor-classification-mri.zip'
out_zip = zipfile.ZipFile(ekstrak_zip, 'r')
out_zip.extractall('/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/Dataset Brain MRI')
out_zip.close()

print('Berhasil Ekstrak ke Dataset Brain MRI')

"""#Preprocessing

###Mengalokasikan ulang Data Train dan Test menjadi 1 kesatuan Dataset
"""

!mkdir "raw dataset"

import shutil

source = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/glioma_tumor (1)"
destination = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/glioma_tumor"

files = os.listdir(source)

for file in files:
	new_path = shutil.copy(f"{source}/{file}", destination)
	print(new_path)

source = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/meningioma_tumor (1)"
destination = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/meningioma_tumor"

files = os.listdir(source)

for file in files:
	new_path = shutil.copy(f"{source}/{file}", destination)
	print(new_path)

source = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/no_tumor (1)"
destination = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/no_tumor"

files = os.listdir(source)

for file in files:
	new_path = shutil.copy(f"{source}/{file}", destination)
	print(new_path)

source = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/pituitary_tumor (1)"
destination = "/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/pituitary_tumor"

files = os.listdir(source)

for file in files:
	new_path = shutil.copy(f"{source}/{file}", destination)
	print(new_path)

import glob 
for path in glob.glob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/glioma_tumor (1)*"):
    shutil.rmtree(path)
for path in glob.glob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/meningioma_tumor (1)*"):
    shutil.rmtree(path)
for path in glob.glob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/no_tumor (1)*"):
    shutil.rmtree(path)
for path in glob.glob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/pituitary_tumor (1)*"):
    shutil.rmtree(path)

"""###Split Dataset dengan Alokasi 80% 10% 10 %"""

!pip install split-folders

import glob
total_image = len(list(glob.iglob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset/*/*.jpg*", recursive=True)))
print("Total Dataset Raw    : ",total_image," JPG Image \n")

!mkdir "New Dataset"

import splitfolders
splitfolders.ratio('/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/raw dataset', output='/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset',
                   seed=82, ratio=(.8, .1, .1), group_prefix=None)

total_image = len(list(glob.iglob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/train/*/*.jpg*", recursive=True)))
print("Total Dataset Train    : ",total_image," JPG Image \n")

total_image = len(list(glob.iglob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/val/*/*.jpg*", recursive=True)))
print("Total Dataset Val    : ",total_image," JPG Image \n")

total_image = len(list(glob.iglob("/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/test/*/*.jpg*", recursive=True)))
print("Total Dataset Test    : ",total_image," JPG Image \n")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os

training_dir = r"/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/train/"
validation_dir = r"/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/val/"
testing_dir = r"/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/New Dataset/test/"

categories = ["glioma_tumor","meningioma_tumor","no_tumor","pituitary_tumor"]

"""####Training data"""

img_size = (128,128)

training_data = []

def create_training_data():
    for category in categories:
        path = os.path.join(training_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                training_data.append([new_array,class_num])
            except Exception as e:
                pass
create_training_data()
print("Berhasil Create Training Data")

validation_data = []

def create_validation_data():
    for category in categories:
        path = os.path.join(validation_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                validation_data.append([new_array,class_num])
            except Exception as e:
                pass
create_validation_data()
print("Berhasil Create Validation Data")

testing_data = []

def create_testing_data():
    for category in categories:
        path = os.path.join(testing_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                testing_data.append([new_array,class_num])
            except Exception as e:
                pass
create_testing_data()
print("Berhasil Create Testing Data")

X_train = []
Y_train = []
for features,label in training_data:
    X_train.append(features)
    Y_train.append(label)
X_train = np.array(X_train).reshape(-1,128,128)
X_train = X_train.astype('float32')/255.0  
X_train = X_train.reshape(-1,128,128,3)
print(X_train.shape)

X_val = []
Y_val = []
for features,label in validation_data:
    X_val.append(features)
    Y_val.append(label)
X_val = np.array(X_val).reshape(-1,128,128)
X_val = X_val.astype('float32')/255.0  
X_val = X_val.reshape(-1,128,128,3)
print(X_val.shape)

X_test = []
Y_test = []
for features,label in testing_data:
    X_test.append(features)
    Y_test.append(label)
X_test = np.array(X_test).reshape(-1,128,128)
X_test = X_test.astype('float32')/255.0  
X_test = X_test.reshape(-1,128,128,3)
print(X_test.shape)

"""####Encoding"""

Y_train = np.array(Y_train)
Y_val = np.array(Y_val)
Y_test = np.array(Y_test)

from keras.utils.np_utils import to_categorical 
Y_train = to_categorical(Y_train, num_classes = 4)
Y_val = to_categorical(Y_val, num_classes = 4)
Y_test = to_categorical(Y_test, num_classes = 4)

print("x_train shape",X_train.shape)
print("x_val shape",X_val.shape)
print("x_test shape",X_test.shape)
print("y_train shape",Y_train.shape)
print("y_test shape",Y_val.shape)
print("y_test shape",Y_test.shape)

"""# HyperParameter Tuning

"""

import tensorflow as tf
from tensorboard.plugins.hparams import api as hp

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

!rm -rf ./logs/

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([512, 1024]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.5))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd', 'rmsprop', 'adamax', 'adam']))

METRIC_ACCURACY = 'accuracy'

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
  )

from keras.preprocessing.image import ImageDataGenerator
batch_size = 32

datagen = ImageDataGenerator(
        rotation_range = 30,
        zoom_range = 0.2,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,  
        vertical_flip=False)

def train_test_model(hparams):

  model = Sequential([
                                      Input(shape=(128,128,3)),

                                      Conv2D(64, (5,5),padding = 'Same', activation = 'swish'),
                                      MaxPooling2D(pool_size = (2,2)),
                                      Dropout(hparams[HP_DROPOUT]),

                                      Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
                                      MaxPooling2D(pool_size = (2,2), strides=(2,2)),
                                      Dropout(hparams[HP_DROPOUT]),

                                      Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
                                      MaxPooling2D(pool_size = (2,2), strides=(2,2)),
                                      Dropout(hparams[HP_DROPOUT]),

                                      Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
                                      MaxPooling2D(pool_size = (2,2), strides=(2,2)),
                                      Dropout(hparams[HP_DROPOUT]),

                                      Conv2D(256, (3,3),padding = 'Same', activation = 'swish'),
                                      MaxPooling2D(pool_size = (2,2), strides=(2,2)),
                                      Dropout(hparams[HP_DROPOUT]),
                      
                                      Flatten(),
                      
                                      Dense(hparams[HP_NUM_UNITS], activation="swish"),
                                      Dropout(hparams[HP_DROPOUT]),
                                      Dense(4, activation="softmax"),
                                      ])
  
  model.compile(
      optimizer=hparams[HP_OPTIMIZER],
      loss='categorical_crossentropy',
      metrics=['accuracy'],
  )

  datagen.fit(X_train)
  history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = 5, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size)
  _, accuracy = model.evaluate(X_val, Y_val)
  return accuracy

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams)
    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=50)

session_num = 0

for num_units in HP_NUM_UNITS.domain.values:
  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
    for optimizer in HP_OPTIMIZER.domain.values:
      hparams = {
          HP_NUM_UNITS: num_units,
          HP_DROPOUT: dropout_rate,
          HP_OPTIMIZER: optimizer,
      }
      run_name = "run-%d" % session_num
      print('--- Starting trial: %s' % run_name)
      print({h.name: hparams[h] for h in hparams})
      run('logs/hparam_tuning/' + run_name, hparams)
      session_num += 1

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # wget -q 'https://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip'
# unzip -q hparams_demo_logs.zip -d logs/hparam_demo

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/hparam_tuning

"""#Skenario 1 | CNN
Model 2 = [ 5 Layer CNN,1 Full Connected Layer , Output 4 Softmax ] \


*   CNN = Dropout 0.2
*   FCL = Dense 512 | Dropout 0.2
*   Optimizer = Adam
"""

import tensorflow as tf

model5 = Sequential(
    [
     Input(shape=(128,128,3)),

     Conv2D(64, (5,5),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(256, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Flatten(),
     
     Dense(512, activation = 'relu'),
     Dropout(0.2),
     Dense(4, activation='softmax')
    ]
)

model5.summary()

from keras.optimizers import Adamax
Adam(learning_rate=0.00146, name='Adam')
model5.compile(optimizer = 'Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])

epochs = 100  
batch_size = 32
datagen = ImageDataGenerator(
        rotation_range=30,
        zoom_range = 0,
        width_shift_range=0,  
        height_shift_range=0,  
        horizontal_flip=True,  
        vertical_flip=False)

from tensorflow.keras import callbacks
filepath="/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/model_5.h5"
checkpoint = callbacks.ModelCheckpoint(
                                       filepath,
                                       monitor='val_accuracy',
                                       verbose=1,
                                       save_best_only=True,
                                       mode='auto',
                                       )

def scheduler(epoch, lr):
  if epoch < 5:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)

import time

start = time.time()
datagen.fit(X_train)
history5 = model5.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size,
                              callbacks=[lr_schedule, checkpoint])
stop = time.time()

print(f" Waktu training {stop - start} s")

# model5.save('model_5.h5')
# print("Berhasil menyimpan model_5.h5")

"""##Plot dan Evaluasi Pada Model 1"""

acc = history5.history["accuracy"]
val_acc = history5.history["val_accuracy"]
loss = history5.history["loss"]
val_loss = history5.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'm')
ax1.set_title('Skenario Model 1 | Accuracy')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'm')
ax2.set_title('Skenario Model 1 | Loss')
ax2.legend(['Model loss','Model Val loss'])

plt.show()

"""Evaluasi Model 1"""

loss, acc = model5.evaluate(X_train,Y_train,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model5.evaluate(X_val,Y_val,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model5.evaluate(X_test,Y_test,verbose = 0)
print("Testing Loss {:.5f} dan Testing Accuracy {:.2f}%".format(loss,acc*100))

import itertools
from sklearn import metrics
from sklearn.metrics import f1_score, recall_score, accuracy_score
from sklearn.metrics import precision_score, confusion_matrix, classification_report

# CR untuk Validation Data
print("model 1")
pred = model5.predict(X_val)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val, labels, target_names = categories))

import seaborn as sns
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.BuPu):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

Y_pred = model5.predict(X_val)
Y_pred_classes = np.argmax(Y_pred,axis = 1) 
Y_true = np.argmax(Y_val,axis = 1) 

confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
print("Model 3")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""Precision Score Model 1"""

Y_pred_train = model5.predict(X_train)
Y_pred_classes_train = np.argmax(Y_pred_train,axis = 1) 
Y_true_train = np.argmax(Y_train,axis = 1) 

ps_train_model3 = precision_score(Y_pred_classes_train, Y_true_train, average='weighted')
ps_val_model3 = precision_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 3:', ps_train_model3)
print('Precision Score Validation Data Model 3:', ps_val_model3)

"""Recall Score Model 1"""

rs_train_model3 = recall_score(Y_pred_classes_train, Y_true_train, average='weighted')
rs_val_model3 = recall_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 3:', rs_train_model3)
print('Precision Score Validation Data Model 3:', rs_val_model3)

"""F1 Score Model 1"""

f1_train_model3 = f1_score(Y_pred_classes_train, Y_true_train, average='weighted')
f1_val_model3 = f1_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 3:', f1_train_model3)
print('Precision Score Validation Data Model 3:', f1_val_model3)

"""#Skenario 2 | CNN
Model 2 = [ 5 Layer CNN,1 Full Connected Layer , Output 4 Softmax ] \


*   CNN = Dropout 0.2
*   FCL = Dense 512 | Dropout 0.2
*   Optimizer = RMSprop
"""

import tensorflow as tf

model2 = Sequential(
    [
     Input(shape=(128,128,3)),

     Conv2D(64, (5,5),padding = 'Same', activation = 'swish'),
     MaxPooling2D(pool_size = (2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'swish'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(256, (3,3),padding = 'Same', activation = 'swish'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Flatten(),
     
     Dense(512, activation = 'relu'),
     Dropout(0.2),
     Dense(4, activation='softmax')
    ]
)

model2.summary()

from keras.optimizers import Adam
RMSprop(learning_rate=0.00146, name='RMSprop')
model2.compile(optimizer = 'RMSprop',loss = 'categorical_crossentropy',metrics = ['accuracy'])

epochs = 100  
batch_size = 32
datagen = ImageDataGenerator(
        rotation_range=0,
        zoom_range = 0,
        width_shift_range=0,  
        height_shift_range=0,  
        horizontal_flip=True,  
        vertical_flip=False)

from tensorflow.keras import callbacks
filepath="/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/model_1_brain_FCL.h5"
checkpoint = callbacks.ModelCheckpoint(
                                       filepath,
                                       monitor='val_accuracy',
                                       verbose=1,
                                       save_best_only=True,
                                       mode='auto',
                                       )
earlystop = callbacks.EarlyStopping(monitor='val_loss',
                                    mode='auto',
                                    patience=4,
                                    restore_best_weights=True)

import time

start = time.time()
datagen.fit(X_train)
history2 = model2.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size,
                              callbacks=[checkpoint])
stop = time.time()

print(f" Waktu training {stop - start} s")

"""##Plot dan Evaluasi Pada Skenario 2"""

acc = history2.history["accuracy"]
val_acc = history2.history["val_accuracy"]
loss = history2.history["loss"]
val_loss = history2.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'm')
ax1.set_title('Skenario Model 2 | Accuracy')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'm')
ax2.set_title('Skenario Model 2 | Loss')
ax2.legend(['Model loss','Model Val loss'])

plt.show()

"""Evaluasi Model 2"""

loss, acc = model2.evaluate(X_train,Y_train,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model2.evaluate(X_val,Y_val,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model2.evaluate(X_test,Y_test,verbose = 0)
print("Testing Loss {:.5f} dan Testing Accuracy {:.2f}%".format(loss,acc*100))

import itertools
from sklearn import metrics
from sklearn.metrics import f1_score, recall_score, accuracy_score
from sklearn.metrics import precision_score, confusion_matrix, classification_report

# CR untuk Validation Data
print("model 2")
pred = model2.predict(X_val)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val, labels, target_names = categories))

import seaborn as sns
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.BuPu):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

Y_pred = model2.predict(X_val)
Y_pred_classes = np.argmax(Y_pred,axis = 1) 
Y_true = np.argmax(Y_val,axis = 1) 

confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
print("Model 2")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""Precision Score Model 2"""

Y_pred_train = model2.predict(X_train)
Y_pred_classes_train = np.argmax(Y_pred_train,axis = 1) 
Y_true_train = np.argmax(Y_train,axis = 1) 

ps_train_model2 = precision_score(Y_pred_classes_train, Y_true_train, average='weighted')
ps_val_model2 = precision_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 2:', ps_train_model2)
print('Precision Score Validation Data Model 2:', ps_val_model2)

"""Recall Score Model 2"""

rs_train_model2 = recall_score(Y_pred_classes_train, Y_true_train, average='weighted')
rs_val_model2 = recall_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 2:', rs_train_model2)
print('Precision Score Validation Data Model 2:', rs_val_model2)

"""F1 Score Model 2"""

f1_train_model2 = f1_score(Y_pred_classes_train, Y_true_train, average='weighted')
f1_val_model2 = f1_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 2:', f1_train_model2)
print('Precision Score Validation Data Model 2:', f1_val_model2)

"""#Skenario 3 | CNN
Model 1 = [ 5 Layer CNN,1 Full Connected Layer , Output 4 Softmax ] \

*   CNN = Dropout 0.2 
*   FCL = Dense 1024 | Dropout 0.5 
*   Optimizer = Adamax



"""

import itertools
from keras.utils.np_utils import to_categorical 
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input
from keras.optimizers import Adam, SGD
from keras.preprocessing.image import ImageDataGenerator

model1 = Sequential(
    [
     Input(shape=(128,128,3)),

     Conv2D(64, (5,5),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(128, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),

     Conv2D(256, (3,3),padding = 'Same', activation = 'relu'),
     MaxPooling2D(pool_size = (2,2), strides=(2,2)),
     Dropout(0.2),  

     Flatten(),
     
     Dense(1024, activation = 'relu'),
     Dropout(0.5),
     Dense(4, activation='softmax')
    ]
)

model1.summary()

Adamax(learning_rate=0.00146, name='Adamax')
model1.compile(optimizer = 'Adamax',loss = 'categorical_crossentropy',metrics = ['accuracy'])

epochs = 100  
batch_size = 32

datagen = ImageDataGenerator(
        rotation_range=30,
        zoom_range = 0,
        width_shift_range=0,  
        height_shift_range=0,  
        horizontal_flip=True,  
        vertical_flip=False)

from tensorflow.keras import callbacks
filepath="/content/drive/MyDrive/Dataset/Dataset Citra 240 - 235/model_1_brain_Net.h5"
checkpoint = callbacks.ModelCheckpoint(
                                       filepath,
                                       monitor='val_accuracy',
                                       verbose=1,
                                       save_best_only=True,
                                       mode='auto',
                                       )
earlystop = callbacks.EarlyStopping(monitor='val_loss',
                                    mode='auto',
                                    patience=4,
                                    restore_best_weights=True)

import time

start = time.time()
datagen.fit(X_train)
history1 = model1.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size,
                              callbacks=[checkpoint])
stop = time.time()

print(f" Waktu training {stop - start} s")

# model1.save('model_1_brain_Net.h5')
# print("Berhasil menyimpan model_1_brain_Net.h5")

"""##Plot dan Evaluasi Skenario 3"""

from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

acc = history1.history["accuracy"]
val_acc = history1.history["val_accuracy"]
loss = history1.history["loss"]
val_loss = history1.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'm')
ax1.set_title('Skenario Model 3 | Accuracy')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'm')
ax2.set_title('Skenario Model 3 | Loss')
ax2.legend(['Model loss','Model Val loss'])

plt.show()

np.save('history_1_adamax.npy',history1.history)
print("Saving Complete")
# history1=np.load('history_1_adamax.npy',allow_pickle='TRUE').item()

"""Evaluasi Model Validasi"""

from sklearn.metrics import f1_score, recall_score, accuracy_score
from sklearn.metrics import precision_score, confusion_matrix, classification_report
from sklearn import metrics

loss, acc = model1.evaluate(X_train,Y_train,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model1.evaluate(X_val,Y_val,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

loss, acc = model1.evaluate(X_test,Y_test,verbose = 0)
print("Testing Loss {:.5f} dan Testing Accuracy {:.2f}%".format(loss,acc*100))

print("model 3")
pred = model1.predict(X_val)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val, labels, target_names = categories))

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.BuPu):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

Y_pred = model1.predict(X_val)
Y_pred_classes = np.argmax(Y_pred,axis = 1) 
Y_true = np.argmax(Y_val,axis = 1) 

confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
print("Model 1")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""Precision Score Model 3"""

Y_pred_train = model1.predict(X_train)
Y_pred_classes_train = np.argmax(Y_pred_train,axis = 1) 
Y_true_train = np.argmax(Y_train,axis = 1) 

ps_train_model1 = precision_score(Y_pred_classes_train, Y_true_train, average='weighted')
ps_val_model1 = precision_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 1:', ps_train_model1)
print('Precision Score Validation Data Model 1:', ps_val_model1)

"""Recall Score Model 3"""

rs_train_model1 = recall_score(Y_pred_classes_train, Y_true_train, average='weighted')
rs_val_model1 = recall_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 1:', rs_train_model1)
print('Precision Score Validation Data Model 1:', rs_val_model1)

"""F1 Score Model 3"""

f1_train_model1 = f1_score(Y_pred_classes_train, Y_true_train, average='weighted')
f1_val_model1 = f1_score(Y_pred_classes, Y_true, average='weighted')
print('Precision Score Training Data Model 2:', f1_train_model1)
print('Precision Score Validation Data Model 2:', f1_val_model1)